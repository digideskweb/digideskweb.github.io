{"response": [{"author_name": "Colin Percival", "gravatar_hash": "b86a032cdc22e3758a45d4e411eb9782", "author_url": "http://www.daemonology.net/blog/", "date": "Feb 19, 2009", "message": "Am I reading this correctly?  You spent $1200 buying a 1TB+ backup device... so that you can back up 5GB per day?  You do realize that using an online backup service would give you greater redundancy at a lower price (since you wouldn't be paying for a TB of unused space), right?\n\n(Disclaimer: I created the Tarsnap online backup system.)"}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Feb 19, 2009", "message": "Colin, the bandwidth costs become prohibitive. We'd need to transfer 5 GB (and growing!) of data per day.\n\n5120 MB per day, 213 MB per hour, 4 MB per minute.\n\nThat's pretty aggressive and would directly affect our monthly hosting costs. In general my strategy is to make larger one time up front investments to avoid recurring monthly costs which are more expensive in the long term.\n\nI agree S3 and other services can make a lot of sense, particularly when you're starting out, but you end up nickling and diming yourself into substantial sums of money over a period of few years.\n\nI'd much rather spend my monthly recurring budgets on paying software developers whenever possible!\n\nRelated:\nhttp://www.codinghorror.com/blog/archives/000783.html"}, {"author_name": "James", "gravatar_hash": "d2cf5d6a92b85eb029a07a15d244305c", "author_url": null, "date": "Feb 19, 2009", "message": "\"In general my strategy is to make larger one time up front investments to avoid recurring monthly costs which are more expensive in the long term.\"\n\nCouldn't agree more with that statement. Reoccurring cost suck. Pretty to pay off what you can when you can."}, {"author_name": "Colin Percival", "gravatar_hash": "b86a032cdc22e3758a45d4e411eb9782", "author_url": "http://www.daemonology.net/blog/", "date": "Feb 19, 2009", "message": "Jeff, if you take the price of bandwidth to be $0.20/GB, it costs $1 for your daily 5GB backup.   That's $365/year, a small fraction of the cost of your rapidly depreciating $1200 server.\n\nOf course, there are other factors to consider; for instance, now that you have a mostly-empty server sitting around, you'll probably find ways that it can be put to use.  On the other hand, your electricity bill is probably a few $/month higher than it would be if you didn't have that server.  On the grasping hand, if you used a good online backup system (*cough* tarsnap *cough*) you probably wouldn't have to upload all 5GB every day, since there's probably large amounts of that data which remains unchanged from day to day."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Feb 19, 2009", "message": "> if you take the price of bandwidth to be $0.20/GB, it costs $1 for your daily 5GB backup\n\nYes, but (using S3 as an example) it also costs per month to *store* the data as well. And the more you store over time, the more it costs!\n\nAlso, we're not charged by the GB, we are charged based on burstable billing at the 95th percentile ( http://en.wikipedia.org/wiki/Burstable_billing )\n\nThe NAS actually puts the drives to sleep when they're not in use, and the power draw of an embedded mobo is quite modest. \n\nWe could come up with more complex procedures, such as 7zip-ing the backups before transferring them across the network, or taking differentials instead of full backups.. but it's so much easier to just blindly XCOPY the whole shebang to a local NAS and not think about it at all. KISS!\n\nDon't get me wrong, I like online backup solutions, I just don't think they're right for us at the moment."}, {"author_name": "Jonesinator", "gravatar_hash": "a4c84496bc4fde4fd0661304e39cd956", "author_url": null, "date": "Feb 19, 2009", "message": "I develop firmware for RAID controllers (not qnap), and man, I must say we've had a couple pretty gnarly data corruption issues with RAID 6.  For a 4-drive system I'd definately trust RAID 5 with a hot-spare before I trusted RAID 6.  The performance would be better since you can skip the Q-parity calculation, and with four drives the probability of a failure before the reconstruction to the hot-spare completed is very slim.  In fact, we artificially enforce a minimum of 5 drives for RAID6 since RAID10 and RAID5 are basically universally better for 4-drive configurations."}, {"author_name": "Kyle Cordes", "gravatar_hash": "5e8f5b2135205ccf851477ef5e98c546", "author_url": "http://kylecordes.com", "date": "Feb 19, 2009", "message": "4 drives... what an adorable, tiny little storage server  :-)\n\nHere is one type of machine we use, for a similar problem but with much more data volume: 10 1 TB drives, in a RAID 6 to give ~8TB capacity.\n\nhttp://kylecordes.com/2008/12/02/big-raid/"}, {"author_name": "Brian Boatright", "gravatar_hash": "f05e3b9aa180fbacdff713e300a7c5f2", "author_url": "http://www.developerbuzz.com", "date": "Feb 19, 2009", "message": "@Colin: I can appreciate your point of view on using an online backup service, but not in this case. They already have physical servers, rack space, and someone who can jump in a car if needed. If they were still renting it would be different. But even then you have an issue of trusting the backup service provider to work and be available when you need it. So what if it costs more to have your backups local and ready."}, {"author_name": "Avner Braverman", "gravatar_hash": "98ee39b4cecc1d64a331a0fe560c3292", "author_url": null, "date": "Feb 19, 2009", "message": "Jeff, you mentioned putting the drives to sleep to save power. I've been working in the storage industry for a while and I know one thing: if you turn off a disk, you never know if it will spin up again. These are delicate creatures. If reliability is what you want, consider leaving the disks working."}, {"author_name": "Lars Wirzenius", "gravatar_hash": "4446f50fa72a1b8fe7fee62381757404", "author_url": "http://liw.fi", "date": "Feb 19, 2009", "message": "Jeff,\n\nif I understand correctly, you do a database dump every day, and copy that over to the backup server. Most of the dump stays the same from day to day, but the total size of the data is about five gigabytes.\n\nDid you consider using something like rsync, which only copies over changes, not the entire file? This might change the numbers for online backups.\n\nNot that I'm saying you chose wrong. I would have chosen the same thing. If I was you, I would additionally do a copy from the backup server to my home, using rsync."}, {"author_name": "Doekman", "gravatar_hash": "b83de2466ffbd28e6493ee2fcd796978", "author_url": null, "date": "Feb 20, 2009", "message": "You never considered log-file shipping, as is done with FogBugz online?"}, {"author_name": "Aaron Digulla", "gravatar_hash": "8a4d6f03a8879432d8563aefbf48e787", "author_url": "http://darkviews.wordpress.com/", "date": "Feb 20, 2009", "message": "Are you sure this thing is configured correctly? You should get *at least* 40MB/s write and 100MB/s read when doing local benchmark and 20MB/s write over a GBit network wire for a large file.\n\nHeck, with my Raid5, I get 533MB/s local read and 100MB/s read over NFS (30-40MB/s write over NFS4)."}, {"author_name": "Aaron Digulla", "gravatar_hash": "8a4d6f03a8879432d8563aefbf48e787", "author_url": "http://darkviews.wordpress.com/", "date": "Feb 20, 2009", "message": "That said, storing a dumb backup of the database is probably not the best way to keep SO alive. Imagine you have a critical failure of your DB. How many hours will it take to reinstall the DB server (software, all the patches, restore the config) and then load the DB backup?\n\nWouldn't it make more sense to install a second DB server on this box which mirrors the live DB as it changes? This way, you could switch to the backup DB in mere seconds."}, {"author_name": "El Chorizo", "gravatar_hash": "7fa37df595f49909996e390c727f1719", "author_url": null, "date": "Feb 20, 2009", "message": "Simply using XCOPY is a bad method of backup process regardless. Are you XCOPY'ing to different directories every time so you can restore to a previous date if something went wrong and it took you days or weeks to notice?\n\nPersonally, I prefer rdiff-backup and of course keep the increments around for at least 30 days.\n\nIn that 30 day period, you should do your sneakernet off site at least once a month. That way, you'll be able to restore by the snap of a finger anything in the previous 30 days... and after that you can restore anything within a +/- 15 day window from years back if necessary.\n\nVery handy... and just like Colin mentioned with tarsnap -- rdiff-backup only transfers increments over the network. We routinely backup 30+ gig's /day over a remote network link this way as the diff's are only in the 300MB/day range.\n\nAlso, since it uses rsync you can limit the speed if you need -- so you don't have to worry about bursting past a 95% range. Set it to 200kb/s max if you wish and no worries and no charges. Very elegant."}, {"author_name": "El Chorizo", "gravatar_hash": "7fa37df595f49909996e390c727f1719", "author_url": null, "date": "Feb 20, 2009", "message": "Aaron,\n\nHaving a replicated database is not a solution either.. unless you couple that with standard backups.\n\nThe replicated DB replicates ALL things.. so if someone issues a bad command to your primary database (or a malicious one) your replicated database will also issue the command and your hosed -- and HOPEFULLY you have a backup besides the replicated DB.\n\nSame situation for RAID1 -- it protects against hardware failure, but not human failure or hack attacks."}, {"author_name": "Guy", "gravatar_hash": "aa81a6370a42b90429370bcf2dfad2e8", "author_url": null, "date": "Feb 20, 2009", "message": "Personally, I think you've gone and \"jumped that shark\" here.  I don't think this solution will be around for very long before you start looking at some crippling capacity and performance issues.\n\nA bad investment of $1200.  For the $1400, you'd have got a second server, that can be setup to actively manage your backups would have been a much better solution.\n\nThe most important feature of a backup / recovery plan is flexibility.  You can't predict what would go wrong so you need to build in flexibility and resilience into your plan.  Never rely on the human factor (too unreliable) and verify and test your recovery scenarios.\n\nQuestion:  Did you ask the SO community for idea's and thoughts before you implemented this solution?"}, {"author_name": "Aaron Digulla", "gravatar_hash": "8a4d6f03a8879432d8563aefbf48e787", "author_url": "http://darkviews.wordpress.com/", "date": "Feb 20, 2009", "message": "El Chorizo,\n\nI believe that only good one off-the-shelf solution exists for user errors: Time machine (Mac, not H. G. Wells). If your database doesn't allow to roll back committed transactions one by one, no backup will make you truly happy."}, {"author_name": "Robert C. Barth", "gravatar_hash": "c23ef823ae0ba03daf7675991deb3083", "author_url": "http://www.norimek.com/blog/", "date": "Feb 20, 2009", "message": "I assume you're backing up over a different NIC than the one that connects the database to the app server?"}, {"author_name": "Robert C. Barth", "gravatar_hash": "c23ef823ae0ba03daf7675991deb3083", "author_url": "http://www.norimek.com/blog/", "date": "Feb 20, 2009", "message": "Oh, also, have you tried out your DRP yet? E.g. knocked down the db server and tried to restore it from the backup? This is the step most people skip and then when it happens, they find (at the worst possible time), that their DRP doesn't work. D'oh!"}, {"author_name": "Humberto Oliveira", "gravatar_hash": "f13378b101d807e85c09531dd68ce4a7", "author_url": "http://holiveira.com", "date": "Feb 20, 2009", "message": "I've been using the Idera SQL Safe to manage the backups of a client database with the same size of yours. \n\nIt's main benefit is the automatic compression of the database, which compacts a 6Gb database to a 1.3 GB file.\n\nI created two backup scripts, one that creates a full backup every night and another that does an incremental every two hours.\n\nI've been using their freeware version and, despite the fact it has no GUI, it has been super easy to work with.\n\nHere is the link http://www.idera.com/Content/Show68.aspx"}, {"author_name": "El Chorizo", "gravatar_hash": "7fa37df595f49909996e390c727f1719", "author_url": null, "date": "Feb 20, 2009", "message": "Aaron,\n\nThink of rdiff-backup as a more powerful CLI version of time machine. However, time machine knows nothing of database schema.. it wouldn't work for what you want. You need a custom script to run mysqldump's at certain intervals.\n\nBTW, mysqldump's (ie. .sql files) compress well. So, as someone else mentioned we run a script to mysqldump all the databases, compress em with gzip, and rdiff-backup them to different servers.\n\nThe .sql files compress well and the actual diff's on the file day to day are quite tiny so we keep the increments for 30 days. You could keep increments for as long as you want, rdiff-backup lets you specify a time period to keep them around. If you've got the space, keep em for 365 days.\n\nThe only problem with this solution is that as your database grows the mysqldump becomes more of an impact. The larger the database the longer the backup time and mysql locks tables during a dump.\n\nThis is where having a replicated server would be handy as you can backup the secondary server and not impact your primary."}, {"author_name": "Brian Duffy", "gravatar_hash": "974938b8250a08cf9e22714aff1f9fcc", "author_url": null, "date": "Feb 20, 2009", "message": "I tend to agree with the crowd here that this isn't a very good backup method, but for different reasons.\n\nThe advantage of online backups from S3 or similar offerings is availablity -- the amount of available storage is only limited by your wallet. The real disadvantage IMO is recovery time -- it takes forever to download your data.\n\nFor the backup scenario that's been described here, there is a very unsexy solution that would actually do a better job -- LTO-4 tape and BackupExec. The ability to do incremental backup would reduce the storage requirement and the offsite storage would be easy -- swap out the tape! You're better off not getting too creative with backup... just get it right!\n\nIt would cost more than $1,200, but you're going to spending more money when this doesn't work out anyway.\n\nAlso, consider the backup strategy. Is it really useful to retain daily snapshots? Will you really have a reason to want to roll Stackoverflow's database back to last Wednesday?"}, {"author_name": "Brent Ozar", "gravatar_hash": "77f776c2eaf0cc691e8a0880bb8a191f", "author_url": "http://www.brentozar.com", "date": "Feb 20, 2009", "message": "It's good to see a healthy debate here that pretty much mirrors the talks that Jeff and I had.  Like he said, things aren't black and white in the backup arena right now.\n\nLars - the backup does change pretty frequently because we do regular index maintenance, which defrags/rebuilds heavily active tables.  This improves performance, but it involves a lot of database log activity.\n\nDoekman - yep, log shipping is on the very near horizon.\n\nAvner - I agree about powering drives down.  In datacenters where I've powered down SANs to do maintenance, we've had somewhere around a .5% (1 in 200) drive failure rate when powering back up.\n\nAaron - yep, those speeds are a concern.  We could have gotten dramatically faster performance even by using another Lenovo RS110 (1u 2-drive) server, and do log shipping with a standby instance of SQL Server on that box.  This was why I was hollering at Jeff, heh - 4 drives that are so slow don't help other than a huge capacity, and StackOverflow doesn't need that kind of capacity.\n\n\nRobert - no, they're using one nic, but it's mitigated by the fact that the QNap is so doggone slow.  They're hoping they won't have network problems since the writes are slow.  I did highly recommend a separate network for that traffic, but we can play that by ear - since it's not the primary storage for that server, it'll be easy to take an outage window and add in another network card and switch.\n\nHumberto - yep, I've had experience with Idera's products before I came to work for Quest Software, the makers of LiteSpeed.\n\nEl Chorizo - I love your sausage, and I mean that in a non-homo-erotic way.  However, if you do a full backup and then do incrementals for 365 days straight, your restore times are going to be staggeringly long.  I might suggest doing fulls weekly, and differentials every day, and incrementals every 15 minutes.  You'll be able to recover much faster."}, {"author_name": "xero", "gravatar_hash": "592cec2cd0e940557786bba006a67d91", "author_url": "http://xerotopia.net", "date": "Feb 20, 2009", "message": "I can't help but think a cheap computer (maybe even with an Intel Atom in it) with a Drobo attached would have been cheaper, easier to maintain, and offer the same if not better performance and maintenance. http://www.drobo.com/"}, {"author_name": "El Chorizo", "gravatar_hash": "7fa37df595f49909996e390c727f1719", "author_url": null, "date": "Feb 20, 2009", "message": "Brent,\n\nYou misunderstand the way rdiff-backup does incrementals. The way rdiff-backup works is this. If you do a full snapshot today (as your most current snapshot is always a full snapshot, the incrementals are previous snapshots) you will have a full .sql file you can restore in a matter of seconds. In fact, you simply need to scp it from your rdiff-backup repository to your mysql system and do an import to your mysql server. Total time is the time it takes to scp the mysql file and however long it takes your mysql server to import the database.\n\nIf you don't want to do it with a mysql file, you can lock the tables and rdiff-backup the actual MYD/MYI files and your restore time is very fast... but you won't get a good compression on that setup as the binary files wont compress and you'll get a ton of extra data (indexes, etc) that you dont' necessarily need... this is going to save restore time at the expense of backup time and space. So, a decision you have to make.\n\nIf I want to restore a database that's a week old, its very simple and rdiff-backup will handle the incrementals for me... I simply need to do a:\n\nrdiff-backup -r 7D host.net::/remote-dir/file /tmp/file\n\nBoom, now I can import my .sql file that shows me an exact snapshot of my mysql dump from 7 days ago.\n\nVery elegant, simple and space efficient -- and protects against even human failure and silent data loss/deletion that a replication server won't cover."}, {"author_name": "AndyV", "gravatar_hash": "d6901d5b97675e18641b19a4347b1383", "author_url": null, "date": "Feb 20, 2009", "message": "Jeff--\n\n  You're not really backing very much up.  You've already got RAID on your main database server and are only piling increasing levels of rendundancy within your colo box.  As that blog site found out, RAID isn't backup.\n\n  The modicum of backup that you have now is actually Geoff storing your data offsite, much too infrequently IMO.  My suggestion to you is to buy Geoff at least one more USB drive.  One of the drives should always be attached inside the colo box.  Daily backups should be written to the drive, and older backups should be deleted in favor of newer data.  Your team should get a daily email about the success or failure of this.\n\n  Once or twice a week, Geoff should stop by the center and swap the drives.  He shouldn't have to do anything else at the time-- this will make an otherwise annoying task much easier for him.  He doesn't have to type anything or wait for anything.  Just swap the drives.\n\n  RAID is such a cool and interesting thing that it deludes-- Consider the situation for the four disks in your NAS box.  How much redundancy do you have *without* RAID?  Without RAID, you can write a dump to each drive.  Now you can now lose *three* disks instead of two and still have a copy.  You also don't have to worry about rebuilding the array or any other RAID weirdness.  What do you lose?  Your files have to be smaller than your disks.  In your case, 5G files to a 640G drive, and you only really care about the last dozen or so days.  RAID isn't backup...\n\nAndy"}, {"author_name": "Eric Z. Beard", "gravatar_hash": "62cc585b9fd3ee7182dadbd09a7f4b47", "author_url": null, "date": "Feb 20, 2009", "message": "Jeff,\n\nI'm using a very similar scheme, with two databases that back up to 80Gb each.  That's just the full nightly backup.  I also keep all of the transaction log backups, which are enormous.\n\nI found that backing up directly to a NAS from Sql Server is slow and unreliable.  The file share will suddenly become unavailable halfway through the backup and it will fail.\n\nWhat I do is write an app on the NAS itself to pull the backups off of the database server.  So, the backup to local disk is fast (8 minutes vs. &gt;1 hour), my last full backup ends up in two physical places, and then there are a week of backups kept on the NAS, plus the USB drive sneakernet.  USB drives are a great offsite solution.  Just be sure to have 2 of them to swap out, so you can start the copy process remotely.  Once you get up to a few hundred Gigs, it takes hours to move all of that over USB."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Feb 20, 2009", "message": "> Just be sure to have 2 of them to swap out, so you can start the copy process remotely.\n\nYes, thanks to you and others who pointed this oit. I belatedly realized that Geoff should have two USB drives for ease of swap/copy last night -- and I ordered him another one! Red and Blue.\n\n> A bad investment of $1200. For the $1400, you\u2019d have got a second server, that can be setup to actively manage your backups would have been a much better solution.\n\nI agree having another identically configured server is super flexible and confers a lot of advantages. I'd be more open to this if I could have built up a server with 4 drive bays, but everything I priced out on newegg ended up not being nearly as good a deal as the 2-bay RS110."}, {"author_name": "Brent Ozar", "gravatar_hash": "77f776c2eaf0cc691e8a0880bb8a191f", "author_url": "http://www.brentozar.com", "date": "Feb 20, 2009", "message": "Right, you don't buy servers on NewEgg.  ;-)  Instead go to Dell.com and you can pick up a 4-bay 1950 with 73gb 10k drives for $2k.  At the moment the Dell Outlet has a couple with the 4-bay 2.5\" backplanes for $1600 with 4gb ram, too.\n\nI'm a fan of the HP DL360's - they take up to 6 2.5\" drives, which makes 'em really flexible SQL Servers.  They cost more, but resellers will get you a 20-30% discount."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Feb 20, 2009", "message": "I will never, ever use Dell servers. Sorry. I hate their \"you're a sucker if you pay the prices listed on our website\" sales philosophy a bit too much. Coupons and salespeople, ugh, I'd rather buy a used car!\n\nI actually found a good barebones Intel base 4-bay server for $520 on newegg, but by the time I put a CPU and memory in it, it's about the same $800.\n\nAnyway, water under the bridge at this point.. I don't think we'll be buying too much more hardware in the next few months unless something unusual happens."}, {"author_name": "Brent Ozar", "gravatar_hash": "77f776c2eaf0cc691e8a0880bb8a191f", "author_url": null, "date": "Feb 20, 2009", "message": "Ah, yeah, that sales model is a pain. They all do it too, and that's certainly not an excuse. It's crappy business, but it's the way enterprise gear works.  Same with software too, even MS stuff with Enterprise-class licensing."}, {"author_name": "ray", "gravatar_hash": "16c337ddb689492848e26040b90bd69b", "author_url": null, "date": "Feb 20, 2009", "message": "Can't believe no one has mentioned this yet, but, Seagate just recently had a problem with most of their 7200.11 drives.\n\nJeff, if I'm reading your screenshot correctly, all those drives (which happen to be what I own) are running SD13 firmware.  You should update them immediately if they match up with Seagate's criteria.\n\nhttp://seagate.custkb.com/seagate/crm/selfservice/search.jsp?DocId=207931"}, {"author_name": "ray", "gravatar_hash": "16c337ddb689492848e26040b90bd69b", "author_url": null, "date": "Feb 20, 2009", "message": "Actually, they have appeared to downgrade the severity of the problem, but it's probably still worth upgrading.  There was great havoc on their forums about a month ago when drives stopped working."}, {"author_name": "Lars Wirzenius", "gravatar_hash": "4446f50fa72a1b8fe7fee62381757404", "author_url": "http://liw.fi", "date": "Feb 21, 2009", "message": "Brent: I'd be curious to know how much the SQL dump actually changes over time. I realize there is a lot of change, but I would guess that much of the data still stays the same from day to day: most posts aren't edited every day, for example.\n\nRather than guessing, do you think we could do an experiment? Something like the following: take two full SQL dumps a few days apart, call them A and B (old and new). Then run (probably on a Linux machine) \"rdiff signature A A.sig\" and after that finishes, \"rdiff delta B B.sig delta.dat\". The size of the delta.dat file is the amount of changes between the two files.\n\nEven if this doesn't affect StackOverflow, it would be useful information for other people following this discussion, thinking about their own database backup solutions."}, {"author_name": "Larry Silverman", "gravatar_hash": "668f1f8d3db8ec0b43cb7285c34908eb", "author_url": null, "date": "Feb 21, 2009", "message": "Others have mentioned it, and I'll also recommend using a third party backup/compression plug-in for SQL Server.  We use RedGate's SQL Backup, it's performed very well for us.  We back up around 100GB daily spread across several dozen databases.  This product supports using multiple threads to increase backup throughput.  Our full nightly backup for all databases runs in under 15 minutes (diffs and logs much faster), and that includes the compression.  I'm getting 3-4X compression ratios (there are different compression levels to balance speed vs. size).  Of course compression is going to vary widely depending on what you're storing, and how much empty space is inside.  Since Stack Overflow is mostly text, I'd expect you'd get some great compression.\n\nPrior to this, I used the built-in SQL Server backups.  They were much much slower and provided no compression.  I added my own compression following the backups.  Tried bz2, zip, tar.gz, 7-zip.  Perhaps I was doing it wrong, but the compression would peg the CPU for much longer than I liked, and if I lowered the priority of the compression job, it would obviously just take longer.  The whole DIY backup/compress cycle took hours for full backups.  The commercial software was well worth it, IMHO."}, {"author_name": "Matt", "gravatar_hash": "6e9fc2629967a6893a162ed52c9e2fa2", "author_url": "http://kriegster.com", "date": "Feb 21, 2009", "message": "hi jeff\n\nSince the box holds just 4 drives and you're only getting 50% of the capacity due to raid6 anyway, why not go with tried-and-true mirroring?  \n\nRegards"}, {"author_name": "Ian Nelson", "gravatar_hash": "a8a9524783c9b49f47b4ba57e0103dd6", "author_url": "http://ianfnelson.com", "date": "Feb 22, 2009", "message": "\"we can store a huge history of daily backups\"\n\nPlease tell me you're backing up more frequently than that? I liked Brent's suggestion of incremental 15-minute backups.\n\nBut more than that, I liked Guy's suggestion of spending $200 more for a second server for an identically configured server."}, {"author_name": "AndyM", "gravatar_hash": "ca95ed9737215505faf4ca931cb0613d", "author_url": null, "date": "Feb 23, 2009", "message": "Jeff,\n\nI own a QNAP-209 with a gigabit NIC. I found that by changing the jumbo frames size (MTU) from 3000 to 9000 dramatically improved my data transfer rates (6 Mb/s -&gt; 20Mb/s). (Note: you have to check the other devices on your network also support jumbo frames and set these too)\n\nThis is because the poor little 500Mhz cpu is your bottle neck rather than disc. Having bigger frames means less work for the cpu and an improved data rate.\n\nHope this helps.\nAndy"}, {"author_name": "Dave", "gravatar_hash": "b1c365e59c0fa6ffd646d25e34def16e", "author_url": null, "date": "Feb 24, 2009", "message": "Matt: The advantage of RAID6 is that it can survive the death of any two drives in the array. With mirroring (I assume you are talking about RAID10 or two separate RAID1 volumes) 2 of the 6 possible combinations of two dead drives will result in loss of the whole array. R10 will perform better then R6 for writes (no extra reads and checksum calcs needed) but offers greater resilience. For a live database R10 is preferred for performance reasons, but for a backup solution resilience trumps performance.\n\n\nJeff: I have to agree that a network based off-site backup would be better. Most of your data won't change with each backup so rsync or similar tools/protocols (IIRC rdiff-backup is based upon rsync) can massively reduce the amount of data you need to transfer each time and if you are worried about breaking through the 95% barrier just set the backup process to a low throttle.\n\nWe backup ~30Gb of source+documents+databases mostly daily (some itimes are synced more than once per day) using rsync and some scripts of my own devising and that takes very little bandwidth. Just to be sure I throttle rsync down to 50Kbyte/sec so (even allowing extra for VPN overhead) it alone never goes anywhere near the 2Mbit \"95th percentile\" billing level that we have for the co-location arrangement in which our backup servers (amongst other services) live.\n\nWe use our own colo+servers because of the sensitivity of some of the data we hold. For the data you hold on this site I'm sure services like http://www.bqbackup.com/ (there are several similar services out there, I only mention them specifically because I've used them for my home backups in the past so the name is in my memory) would be suitable and, depending how many backups you want to keep in your cycle, possibly much cheaper than arranging your own colo+kit."}, {"author_name": "Tim", "gravatar_hash": "c0f15ca8b9f288f23622eff79eaed432", "author_url": null, "date": "Feb 24, 2009", "message": "@Jeff Atwood\n\nYou might want to check out SiliconMechanics.com.\n\nThey pre-build your server and are typically LESS expensive than NewEgg.\n\nI built a 1.8 GHz AMD, 1 GB RAM, 4x1TB disk for just $1788.00."}, {"author_name": "asdf", "gravatar_hash": "6a1808b3927634bbc70574c224f498a3", "author_url": null, "date": "Feb 25, 2009", "message": "what are the benefits of using RAID-6 at all in this backup scenario? imho RAID is useful on systems which are live where you need 100 percent uptime paired with good performance, not for backing up the backup disks.\n\nI would use disks from different vendors without any raid. just xcopy the backup from over network share to one of those disks and xcopy locally from that disk to the other disks. in the meantime compress the backup. optionally upload to some remote backup store.\n\nthen if you got n number of disk you can compensate n-1 drive failures and you can't run into RAID-specific problems."}, {"author_name": "Jess Sightler", "gravatar_hash": "eac4001b8c054ee994480f0a6e2cdcf8", "author_url": "http://www.jroller.com/jsight/", "date": "Feb 25, 2009", "message": "I agree with most of this setup, except for the bi-monthly, physical offsite copies.\n\nWhy not just automate this with rsync?  Its only bimonthly, so you can set the bandwidth low so that it doesn't impact your bandwidth bill at all.  It seems really silly to do this any other way."}, {"author_name": "Brent Ozar", "gravatar_hash": "77f776c2eaf0cc691e8a0880bb8a191f", "author_url": "http://www.brentozar.com", "date": "Feb 25, 2009", "message": "asdf - there's no benefits to raid 6 in this backup scenario.\n\nLars - around 50% of the database changes per day.  A big chunk of that is due to index defrags - to get really screaming fast performance on SQL Server, you want your indexes defragmented regularly.  Every day, it's defragging everything over 10% fragmented, and rebuild everything over 30% fragmented.  StackOverflow is pretty heavily indexed in order to make some of the more challenging queries return fast, and it definitely pays off in terms of query runtimes.\n\nLarry - none of the third party compression products are able to match 7zip when it comes to output size.  7zip kills your CPUs while you compress the backup, but the payoff is that you transmit less data over the wire out of the colo facility.  That's the expensive part - network bandwidth - so they don't mind doing extra CPU compression."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Feb 26, 2009", "message": "Based on feedback so far:\n\n1) I turned off the drive sleep option.\n\n2) I am doing full uncompressed backups every day, and incremental compressed backups every 4 hours.\n\n3) We bought Geoff two 500 GB USB hard drives so he can swap one for the other every time he visits.\n\nThe NAS does a little better (15%) on read/write throughput in the datacenter, due to the different network cards and better switch compared to my home router. For large single file:\n\nwrite ~ 8.6 MB/sec\nread ~ 27 MB/sec"}, {"author_name": "Ckris", "gravatar_hash": "fbe812c0a1c0a03be874a7196f18475e", "author_url": null, "date": "Feb 26, 2009", "message": "Not related to backup systems per se, but here goes an observation about software RAID in Linux.\n\nRecently my friend configured a software raid for plain raid-1 configuration with Linux device mapper as present in 2.6.18 kernel which is the standard one in Debian 4.0 \"Etch\" distribution. After a while system started to behave silly, turned out one of the drives died. That is, the first one. Theoretically that should make no difference, the system disables damaged drive and stays in order with no change in behaviour sans possibly issuing a warning. But that was not the case -- every read request concluded with a failure. System had to be reconfigured manually to remove failed drive and rebooted from the second. That was due to the raid-1 implementation in Linux 2.6.18 device mapper, where in the drivers/md/dm-raid1.c file one can find the following:\n\nstatic void do_reads(struct mirror_set *ms, struct bio_list *reads)\n{\n    region_t region;\n    struct bio *bio;\n    struct mirror *m;\n\n    while ((bio = bio_list_pop(reads))) {\n            region = bio_to_region(&amp;ms-&gt;rh, bio);\n\n            /*\n             * We can only read balance if the region is in sync.\n             */\n            if (rh_in_sync(&amp;ms-&gt;rh, region, 0))\n                    m = choose_mirror(ms, bio-&gt;bi_sector);\n            else\n                    m = ms-&gt;default_mirror;\n\n            map_bio(ms, m, bio);\n            generic_make_request(bio);\n    }\n}\n\nOK. Quite well. As can be seen, the acting mirror is choosen by the function choose_mirror. OK, but then that function looks like this:\n\nstatic struct mirror *choose_mirror(struct mirror_set *ms, sector_t sector)\n{\n    /* FIXME: add read balancing */\n    return ms-&gt;default_mirror;\n}\n\nThat is, not only system won't read from both mirrors to possibly improve preformance, but will also get stuck if the `default' device choose to die. To my knowledge this was fixed only recently with 2.6.26 kernel."}, {"author_name": "Dave", "gravatar_hash": "b1c365e59c0fa6ffd646d25e34def16e", "author_url": null, "date": "Feb 27, 2009", "message": "Brent Ozar: I'm pretty sure I don't agree with there being no benefit for RAID in a backup volume...\n\nCkris: Interesting. After reading that I did a completely unscientific performance test against one of my RAID1 volumes and the constituent drives and found that while reads were balanced over the two drives there was no actual performance gain (the RAID volume performed no faster than the fastest individual drive). In the period following ~1445 in the graph at http://home.spillett.net:81/t/io_md0-hour.png I ready 20Gb from each drive followed by from the RAID volume that are used in. This is with kernel 2.6.24 as found in debian/etch-n-half. hda and hdc are on separate controllers, so the bottleneck shouldn't be there. Not that I mind overly, I'm using R1 for redundancy not performance, but I'll probably investigate more thoroughly (different read block sizes, concurrent reads, ...) later."}, {"author_name": "cdonner", "gravatar_hash": "d22866cc1cf344f256ae12a92392c36a", "author_url": "http://cdonner.com/more-synology-rackstation-network-tests.htm", "date": "Feb 28, 2009", "message": "I have a Synology RS-407, a very similar NAS device, and can confirm AndyM's observation regarding jumbo frames. Setting the MTU size to 9000 roughly doubled my transfer rates."}, {"author_name": "Freehill Media Website Design", "gravatar_hash": "52bfdeaaa81edd3e44d6c5434445c8d5", "author_url": "http://www.freehillmedia.com", "date": "Apr 01, 2009", "message": "@Colin I've been down the road of online storage for backups, it's not fun when you need to transfer all that data back immediately\n\nWe have different levels and different ends of the building for our backup systems to the primary servers.  We utilise different switches, and NICs for the backups so it doesn't affect users on the sites."}]}