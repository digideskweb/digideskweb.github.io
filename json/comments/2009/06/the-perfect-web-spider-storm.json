{"response": [{"author_name": "Mo", "gravatar_hash": "d3943c342aebe545668fb9dde94ca2a9", "author_url": null, "date": "Jun 15, 2009", "message": "What is Yahoo Slurp?"}, {"author_name": "Stu Thompson", "gravatar_hash": "428344688dc21d72a3723d94bed66680", "author_url": "http://www.dietpizza.ch/geekomatic", "date": "Jun 15, 2009", "message": "The Yahoo indexing spider software."}, {"author_name": "Mo", "gravatar_hash": "3db410ba43388fb9bd4df8b80956a838", "author_url": "http://nevali.net", "date": "Jun 16, 2009", "message": "It's probably worth sending an e-mail to network-abuse@cc.yahoo-inc.com regarding Slurp\u2019s bombardment, though I wouldn't hold out much hope of it doing much good."}, {"author_name": "Jerry Atric", "gravatar_hash": "1aedb8d9dc4751e229a335e371db8058", "author_url": null, "date": "Jun 16, 2009", "message": "So how are you going to IP ban them? Using IIS or some other method?"}, {"author_name": "Maine-Exista", "gravatar_hash": "6e766e248b8f02ba35c7b3edd67b00d3", "author_url": "http://www.google.com/profiles/maineexista", "date": "Jun 16, 2009", "message": "hey, can ya tell us how did you told to Google \"to send no more than 2 Googlebot search engine spider requests per second\".\n\nThanks for the post"}, {"author_name": "jake", "gravatar_hash": "87ea936495cdab72ce5965f2dbc7e601", "author_url": null, "date": "Jun 16, 2009", "message": "Jeff,\ncan't you let them hit you big time periodically instead of constantly using up your bandwidth to index the updates? please show the whole week.\n\nif they are only going to hit you once a week, i think you should let them. on the understanding that they wont take up as much bandwidth next time.\n\nalso, could you check and see what server event triggered this? could be Steve Jobs now with time on his hands opened an account.\n\nwhat is the bing traffic like?"}, {"author_name": "Gabri van Lee", "gravatar_hash": "56917ca081a34fd90b751f2e278acadd", "author_url": "http://www.webberlog.com", "date": "Jun 16, 2009", "message": "Another interesting thing is the complete drop to zero around 19:15... what happened?"}, {"author_name": "PK", "gravatar_hash": "8b9a3d325d32a1dfed22022800e07a3f", "author_url": null, "date": "Jun 16, 2009", "message": "Gosh i wish Cacti doesn't require PHP and mySQL :( Where's the .NET port of Cacti?! :) Must. Have. Port...."}, {"author_name": "Jon Skeet", "gravatar_hash": "6d8ebb117e8d83d74ea95fbdd0f87e13", "author_url": "http://csharpindepth.com", "date": "Jun 16, 2009", "message": "If you told Google to send no more than 2 requests per second, that's no more than 172,800 requests per day. It sounds like the problem is you don't actually want to handle as much traffic as that. Work out how much you actually want to handle and reduce the crawl rate further."}, {"author_name": "Paolo Bergantino", "gravatar_hash": "80ae8ec34e9e3e66c5be50733afaa44e", "author_url": null, "date": "Jun 16, 2009", "message": "I noticed the site was crawling around the time the graph spikes, so I guess that solves that mystery..."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Jun 16, 2009", "message": "@Maine-Exista\n\n\nMore control of Googlebot's crawl rate\nhttp://googlewebmastercentral.blogspot.com/2008/12/more-control-of-googlebots-crawl-rate.html"}, {"author_name": "matt", "gravatar_hash": "1c6f5b01e73791d1238c7c772000a279", "author_url": null, "date": "Jun 16, 2009", "message": "@PK\nwhat's wrong with PHP &amp; MySQL? It's just software dependencies. You don't have to code in it. Unless it's an ops requirement or something, it's silly to lose out of a great technology like cacti because you don't want to use php/MySQL."}, {"author_name": "Geoff Dalgas", "gravatar_hash": "b437f461b3fd27387c5d8ab47a293d35", "author_url": null, "date": "Jun 16, 2009", "message": "During the hour of 8:00am - 9:00am on 6/15/2009 googlebot hit us 62,100 times is a rate of 17.25 requests per second. Google Adsense (user agent: Mediapartners-Google) hit us 34,308 times at a rate of 9.53 per second during the same timeframe."}, {"author_name": "Dave Cheney", "gravatar_hash": "fda08117336cfde6562315df04b976e8", "author_url": null, "date": "Jun 16, 2009", "message": "From memory, Slurp! doesn't use HTTP/1.1 so to add insult to injury, you have to serve data to it uncompressed."}, {"author_name": "Phil Hannent", "gravatar_hash": "78a6050ffe684c015ae1edf0984b3948", "author_url": null, "date": "Jun 16, 2009", "message": "How does the sitemap file fit into this?\n\nIt has a last updated time, surely if you regenerated that file every 60 seconds (for example) the bots would know which pages had been added/updated and just read those?"}, {"author_name": "Shashi", "gravatar_hash": "68a274b3e5f5b7d993f4a2341ae1a339", "author_url": "http://www.bandhan.com", "date": "Jun 16, 2009", "message": "Do these bots hurt the experience of regular users of the site (by consuming excessive cpu or disk activity)? If that's not the case, I will let it index the site. 1GB bandwidth costs a few cents only."}, {"author_name": "Jon Skeet", "gravatar_hash": "6d8ebb117e8d83d74ea95fbdd0f87e13", "author_url": "http://csharpindepth.com", "date": "Jun 16, 2009", "message": "@Shashi: Whether it was due to the bots or not, SO did seem to be crawling (no pun intended) yesterday."}, {"author_name": "Antony", "gravatar_hash": "c6288867d2338707e2f8566e18c4c651", "author_url": null, "date": "Jun 16, 2009", "message": "What's the ratio of traffic between 'local time' (local to the server) and international? Is it really possible to find an off-peak time these days?"}, {"author_name": "John Mueller", "gravatar_hash": "b43ad15cdfa960b713658e245353e236", "author_url": "http://www.google.com", "date": "Jun 16, 2009", "message": "Hi Jeff\nI am a Webmaster Trends Analyst at Google Switzerland. Looking at your numbers, they appear to more or less match up with the 1 request/second that you specified: 87,098 requests on that day lines up fairly well with 86,400 seconds/day. However, looking at your chart, it appears that you are looking at the bandwidth, not at the number of requests. In this case, I don't know how search engine crawlers would be able to know in advance how much data you are serving per URL. All they can do is throttle the number of requests. In a case like this - assuming you can't significantly change the size of your pages - it may make sense to throttle the number of requests even more."}, {"author_name": "vladd", "gravatar_hash": "334ded7ec95d60a1dc8b986455f8d4bd", "author_url": null, "date": "Jun 16, 2009", "message": "Also, your article says that Log Parser returns 87098 (combined) for Google Crawl for the day. I highly doubt by looking at your graph that out of those 87098, 62100 were made in a single hour as Geoff's comment above suggests. You probably made an error when computing one of those two numbers."}, {"author_name": "Brian Sullivan", "gravatar_hash": "3184c0c8926dde9558fdb7facdadf530", "author_url": null, "date": "Jun 16, 2009", "message": "I know of a site that backends a mobile phone app. \n\nEvery now and then a users sets the commuincation time from 4 times a hour to 4 times a second. \n\nFor each session they keep how may times they have talked to you in the last 5 minutes ( 5 rolling one minute buckets) if the total 5 minute request rate exceeds a configurable limit, they introduce a time lag.\n\nGiven most requests are of the form, this is what I know has chnaged, what do you know has changed. They app still works and load greatly reduced. \n\nCould you not do something similar for spiders?"}, {"author_name": "Andrei Rinea", "gravatar_hash": "ea70b38ea893494ee0cd177adc0f54e5", "author_url": "http://blog.andrei.rinea.ro", "date": "Jun 16, 2009", "message": "No mention of \"Bing!\" ?"}, {"author_name": "Konstantin", "gravatar_hash": "583a23e1ceaca8902ab9182c965766bb", "author_url": "http://page2rss.com", "date": "Jun 16, 2009", "message": "Those numbers don't add up for me.\n\nYou've got 2.5GB consumed by your top three crawlers.\n10 Mbit/sec is roughly 1MB/sec. It's 3.6GB per hour. For a 12 hours that would be more than 40GB.\n\nLooks like the crawlers are not ones to blame."}, {"author_name": "Bill", "gravatar_hash": "690df524b8dd8e5d5cbf268e5bdd531f", "author_url": null, "date": "Jun 16, 2009", "message": "Hmmm... something is off.  There's nothing to suggest the crawlers *aren't* doing what Jeff requested.\n\nFirst off:\n\n56,579 / (24 * 60 * 60) = a crawl rate of 0.65 requests/second\n\nEven if was only during the spike (which nothing indicates is true), it's still under the requested rate of 2 requests/sec:\n\n56,579 / (11 * 60 * 60) = 1.42"}, {"author_name": "Ludwig Weinzierl", "gravatar_hash": "7020d1a898d70abed75697a9a65190b4", "author_url": "http://weinzierl.name", "date": "Jun 16, 2009", "message": "From a user point of view:\nWhen I posted a <a href=\"http://stackoverflow.com/questions/961942/what-is-the-worst-programming-language-you-ever-worked-with\" rel=\"nofollow\">question</a> at stackoverflow last week I did a search for the title just a few minutes after hitting submit.\nAnd guess what, google had it already in the search results! \nFrom the users point of view the high crawl rate is a good thing."}, {"author_name": "Twiggy", "gravatar_hash": "b5acbbd0ae1197801536dc1d17dc3846", "author_url": "http://none", "date": "Jun 17, 2009", "message": "Why not change the model?\nInstead of them pulling the data, why not push it to them?"}, {"author_name": "InSciTek Jeff", "gravatar_hash": "3c7db1077ce759e22c54d235054182dc", "author_url": null, "date": "Jun 20, 2009", "message": "@Twiggy - It's called ABUSE. ie: Google wants to index what IS on your web site, not what you want Google to think is on your web site."}, {"author_name": "Geoff Dalgas", "gravatar_hash": "b437f461b3fd27387c5d8ab47a293d35", "author_url": null, "date": "Jun 25, 2009", "message": "After more detailed analysis we have determined the root cause for this bandwidth storm to be hundreds of requests for our sitemap.xml by 200+ random clients distributed around the globe - most of which were uncompressed.  Below is a breakdown by hour (in GMT) and bytes downloaded from our IIS logs.\n\nHour\tRequests\tBytes\n0\t7\t14,410,605\n1\t5\t10,293,442\n2\t5\t8,496,912\n3\t1\t2,058,692\n4\t8\t16,469,536\n5\t6\t10,293,961\n6\t7\t12,549,318\n7\t6\t12,352,710\n8\t11\t22,645,677\n9\t17\t32,935,566\n10\t5\t10,292,298\n11\t8\t16,467,624\n12\t12\t84,446,015\n13\t16\t158,868,765\n14\t13\t129,393,782\n15\t23\t200,714,100\n16\t17\t151,256,703\n17\t12\t115,704,597\n18\t7\t69,668,249\n19\t9\t88,075,000\n20\t12\t109,470,803\n21\t9\t80,007,480\n22\t5\t49,758,915\n23\t7\t61,611,242\n\nGooglebot is obeying the rate limit of a max of 2 requests per second.  Yahoo accounts for 1% of our traffic so we have since banned yahoo slurp.  We have also deployed measures to limit the number of downloads of sitemap.xml"}]}