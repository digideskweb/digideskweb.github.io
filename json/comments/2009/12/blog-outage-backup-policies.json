{"response": [{"author_name": "Joey Robert", "gravatar_hash": "4f1525e94613b24ac74ab5fa8b7d76fa", "author_url": "http://joeyrobert.org/", "date": "Dec 12, 2009", "message": "Glad you got the backups up and running. It would be really ironic had you lost the data, considering the posts you've made on the subject!"}, {"author_name": "Bill", "gravatar_hash": "690df524b8dd8e5d5cbf268e5bdd531f", "author_url": "http://wdr1.com/blog/", "date": "Dec 12, 2009", "message": "You're missing one very important step:\n\nTEST YOUR RECOVERY PROCESS\n\nPretend your data server just disappeared into a blackhole.  Now try to recover.  \n\nIf you can't, you don't have a good backup process.\n\nThis where Joel's post on five why's would have been really helpful."}, {"author_name": "mgb", "gravatar_hash": "7010f170ec917b56e173f3978c459e0e", "author_url": null, "date": "Dec 12, 2009", "message": "have you tried recovering any of these 'backups'?"}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Dec 12, 2009", "message": "mgb, it's in the post:\n\n> 7. We regularly download the latest database backups and restore them locally (we develop against live data all the time), so we know our backups work. \n\n> Pretend your data server just disappeared into a blackhole. Now try to recover. \n\nWe do this every time we provision a server, which we've done 6 times now. On top of that, you may be interested in this blog post:\n\nhttp://blog.stackoverflow.com/2009/03/tuesday-outage-its-raid-tastic/"}, {"author_name": "BobbyShaftoe", "gravatar_hash": "526f22eb98e81ea305bdcf777e4f733d", "author_url": null, "date": "Dec 13, 2009", "message": "I don't think people need to you to refer them to previous blog posts. It seems a little condescending. Instead of being so quick to defend, try to listen a little. :) Clearly, you lost some data to not being rigorous with your backup situation and I don't think it was just because blog.stackoverflow.com was a low priority. I doubt codinghorror.com was a low priority. :)"}, {"author_name": "fabrizio", "gravatar_hash": "4720c6000a5b3a5c5cf59c3544ff9ab7", "author_url": null, "date": "Dec 13, 2009", "message": "You do 3 full backup. And no differential backups. Why?\nIMHO it's better a full every day + a diff every hour."}, {"author_name": "Alex", "gravatar_hash": "52085aed08baca3cef862d69b897a98b", "author_url": null, "date": "Dec 13, 2009", "message": "fabrizio, Why do diff backups when full backups can easily be done, with plenty of cheap disk space?\n\nMore redundancy is good."}, {"author_name": "Nick Johnson", "gravatar_hash": "dc69af7dd7151a00843602888638fabb", "author_url": "http://blog.notdot.net/", "date": "Dec 13, 2009", "message": "So your only offsite backup is anything up to \"a few weeks\" out of date, and is stored only a mile from the datacenter? Color me unimpressed."}, {"author_name": "Dennis", "gravatar_hash": "a85af5779b78386bc17a9532a5126ecb", "author_url": "http://blog.denniswilliamson.us", "date": "Dec 13, 2009", "message": "I'd like to hear a lot more about the problems at the hosting company."}, {"author_name": "InSciTek Jeff", "gravatar_hash": "3c7db1077ce759e22c54d235054182dc", "author_url": null, "date": "Dec 13, 2009", "message": "The described backup process seems fairly reasonable to me (with one exception below), as long as the tested recovery process really does prove you can restore everything back to normal starting with nothing except the backup files and the restore software application and this verification is getting done periodically.\n\nMy exception to the above is that it would seem like you would want to have a longer backup archive than one USB swap disks worth and be able to go back to the state of everything on a monthly basis for perhaps 1 year or more back. It is really unlikely that you would ever need it, but it is also cheap to have and it becomes critical if some subtle corruptions are discovered at some point or if all of a sudden recent backups are bad due to intermittent hardware problems or backup software bugs, etc."}, {"author_name": "CesarB", "gravatar_hash": "14ab1ae9d0573438fdda37fdf8be7887", "author_url": "http://stackoverflow.com/users/28258/cesarb", "date": "Dec 13, 2009", "message": "I did not see a link to them above, so for completeness some related questions on the stackoverflow sites about this event:\n\nhttp://meta.stackoverflow.com/questions/32676/stack-overflow-blog-down\nhttp://superuser.com/questions/82036/recovering-a-lost-website-with-no-backup"}, {"author_name": "Rob Gomes", "gravatar_hash": "9d99ad1d6d2e7eddb03f90fd05e1fca6", "author_url": null, "date": "Dec 13, 2009", "message": "Jeff,\n\nHave you considered creating scripts on the StackOverflow backup server(s) to do regular automated restores, instead of just using them as cold spares?  \n\nTo echo Nick Johnson's sentiment, I'd strongly consider some automated offsite backup as well.  Seriously.  Think of all the benefits you have from your build system when it comes to releases and rolling back versions - your backup/restore system should be equally robust."}, {"author_name": "Joel Spolsky", "gravatar_hash": "d77663c217e9d27b725338c06674c94b", "author_url": "http://www.joelonsoftware.com", "date": "Dec 13, 2009", "message": "The weak point here is the offsite backup. If PEAK has another major outage we could lose weeks of data (depending on when Geoff last picked up the USB disk), and it may take days to reconstruct the site elsewhere. \n\nThose USB disks are notoriously flaky, too. The number of things that can go wrong without your noticing is large. For example if you have a FAT-formatted USB disk, the most common case, file sizes are limited to 2GB. You may suddenly discover that all the backups on the USB disk are corrupt when the backup file goes over 2GB. That's just one example of \"media failure\" which is why any backup-to-media strategy needs to include scheduled restores, not just occasional restores to developer machines.\n\nRather than the USB copy, you should be doing daily offsite backups to a different data center that you can restore onto quickly.\n\nFor example, automatically copy those full backups over the internet to Amazon S3, and have a standby system on Amazon EC2 ready to serve the site as a backup. The EC2 site should be a warm backup, so that really all you have to do is change a couple of things in DNS and we're back up.\n\nSince copying a couple of gigs a day up to Amazon uses a lot of bandwidth, differential backups start to make more sense."}, {"author_name": "fabrizio", "gravatar_hash": "4720c6000a5b3a5c5cf59c3544ff9ab7", "author_url": null, "date": "Dec 13, 2009", "message": "@Alex: AFAIK diff backups require less time than a full backup. So you can schedule a 1 hour diff backup or less.\nAnd if you need to move you data to another location, a diff backup is smaller  (as also Joel noted)."}, {"author_name": "John Topley", "gravatar_hash": "3b687a35025c05dbc7627af428dff0da", "author_url": "http://johntopley.com/", "date": "Dec 13, 2009", "message": "Jeff should listen to Joel more often. After all, Fog Creek have a few data centres in play for FogBugz On Demand and Copilot, so Joel knows what he's talking about.\n\nFor a site that does the numbers that Stack Overflow does you really don't want to be futzing around with USB drives. What happens if Geoff gets sick and can't travel to the data centre? Your backups should be totally automatic, not reliant on human intervention."}, {"author_name": "mgb", "gravatar_hash": "7010f170ec917b56e173f3978c459e0e", "author_url": null, "date": "Dec 13, 2009", "message": "Another 'offsite backup issue'\nWe have had cases where we couldn't get to our office for a few days because of an incident in another building on the same site.\nif there is a fire or an accident at your ISP you might not be able to get in because of safety issues or an investigation even if your data is undamaged.\n\nA few banks in London found this after the IRA bombing, they had a fully ready backup operation site - but it was inside the same security cordon and nobody could get to it."}, {"author_name": "Jesse Hartwick", "gravatar_hash": "cfd1684ea3df74d506f58377b6da5a72", "author_url": "http://www,appscanadian.ca", "date": "Dec 13, 2009", "message": "I'm a serious noob (don't fully backup my home machine(s), nor backup my blog really at all since none of those things are particularly important to me), but even I saw a giant hole in your plan.\n\nYou say that Geoff lives a mile away from your data center, but only picks up the USB drive every couple of weeks. This means that for a few weeks at a time, you essentially have no offsite backups at all for any of your incremental data (not to mention the fact that USB drives, in my experience, can be rather unreliable).\n\nLet's say Geoff plans to go pick up the drive tomorrow (for some sane value of 'tomorrow') and there's a fire at the data center. It's unlikely, but it could happen. Now your server and your backups are all gone.\n\nAlso, your USB is connected to your database server. I don't know how possible this is, but isn't it possible for there to be a power surge or something which knocks both of these out at once (surge into the server, and it fries the USB drive along with it..)\n\nSeems like there are a lot of ways this could go wrong.\n\nI fully agree with Joel here that your best bet is regular differentials (and occasional full) backups sent to S3, in addition to Geoff picking up the drive a little more frequently than a few weeks. Hell, with something as important as StackOverflow, since the four of you are all so damn far apart, you could get extra paranoid and download the whole shebang to each of your houses on a rotating basis.\n\nI could be talking out of my ass (I did say I was a total noob), but that's my rant."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Dec 13, 2009", "message": "> If PEAK has another major outage we could lose weeks of data\n\nWhat? How is that even possible? The only scenario where we would \"lose weeks of data\" is if the datacenter was hit by a meteor or burned to the ground.\n\nIn the more typical case, where the datacenter became unavailable over the internet, Geoff would drive 1 mile and pull the files off the server or the USB drive. Total elapsed time: a few hours.\n\nI mean, I'm all for redundancy, but what if the Planet Earth is destroyed? Shouldn't we have off-planet backups as well?\n\n> Those USB disks are notoriously flaky, too. The number of things that can go wrong without your noticing is large.\n\nThis seems a little hysterical to me. We've restored from the USB drives before and haven't had a problem. That said, I'll go ahead and order 4 more USB drives -- we'll hook up two of them simultaneously at once and back up to both in parallel (same backup proces, repeated), for extra redundancy. \n\nEDIT: just ordered four samsung 640 GB USB drives, $99 each.\n\n> For example, automatically copy those full backups over the internet to Amazon S3, and have a standby system on Amazon EC2 ready to serve the site as a backup.\n\nWe might be able to upload one backup per week to another datacenter during slack time on Saturday and Sunday, but as a daily task it's a no-go -- we'd have to upload 10 GB+ per day."}, {"author_name": "Joel Spolsky", "gravatar_hash": "d77663c217e9d27b725338c06674c94b", "author_url": "http://www.joelonsoftware.com", "date": "Dec 13, 2009", "message": "\"Shut up. I know things. You will listen to me. Do it anyway.\"\n\nWhat's wrong with transferring 10 GB a day? Amazon charges about $1 to transfer 10 GB. And it's automatic, no USB drives. It shouldn't slow down the live service at all, because you're transferring from the NAS, not the live servers.\n\nAutomatic backup to a server offsite is a totally standard part of a good backup solution."}, {"author_name": "Adam Craig Johnston", "gravatar_hash": "61c3d9d7d4a6213d61648a8479e00703", "author_url": null, "date": "Dec 13, 2009", "message": "Also look into Amazon S3 backup I'm storing 80G for around $20 USD per month."}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Dec 13, 2009", "message": "Sounds like the solution, then, is to give Fog Creek access to FTP in every week and download the database backups every week?\n\nThat way the data is in another data center (aka Fog Creek in NYC) every week, on top of ...\n\n<ul>\n<li>backed up to a dedicated backup device in the rack every 4 hours\n<li>backed up to two external USB drives (100% duplicate) every day\n<li>backed up to LegalTorrents.com (data subset) every month\n<li>USB drives taken offsite every 2/3 weeks\n</ul>\n\nOf course, our contingency planning for when the Earth is destroyed is still woefully inadequate. We really need to get those lunar backup plans going. Although you have to wonder if something is big enough to blow up the earth, it might take out the moon, too."}, {"author_name": "John Mullins", "gravatar_hash": "96fa96a8191fb0dc0eb3fcd1b50a2149", "author_url": null, "date": "Dec 13, 2009", "message": "So, hang on - you switched FROM windows TO linux?! God, that must have hurt your pride ..."}, {"author_name": "Jesse Hartwick", "gravatar_hash": "cfd1684ea3df74d506f58377b6da5a72", "author_url": "http://www,appscanadian.ca", "date": "Dec 13, 2009", "message": "&gt; Sounds like the solution, then, is to give Fog Creek access to FTP in every week and download the database backups every week?\n\n&gt; That way the data is in another data center (aka Fog Creek in NYC) \n\nActually, yes. That does sound like a pretty reasonable solution, though 'every week' still sounds like a long time.\n\n&gt; # backed up to a dedicated backup device in the rack every 4 hours \n&gt; # backed up to two external USB drives (100% duplicate) every day \n\nBoth of which could be destroyed by a sufficiently sized issue at the data center, not that anything like that has ever happened before:\n\nhttp://www.datacenterknowledge.com/archives/2008/03/31/fire-destroys-wisconsin-data-center/\n\n\n&gt; # backed up to LegalTorrents.com (data subset) every month\n&gt; # USB drives taken offsite every 2/3 weeks \n\nThe torrent isn't a complete record, and I doubt you have a way to easily dump the data back into the database (I could be wrong, of course). There is also a large window in both of these options for something to go wrong. If something catastrophic happens a couple days before Geoff picks up the data, your best case scenario is two/three weeks of lost data (or having to transform the torrent data into something usable, yet still outdated).\n\n&gt; Of course, our contingency planning for when the Earth is destroyed is still woefully inadequate.\n\nhttp://en.wikipedia.org/wiki/Straw_man\n\nIf Earth is destroyed, no one will care about StackOverflow being down.\n\nIf the US is nuked to hell, no one will care that StackOverflow is down.\n\nIf your data center burns down, and you don't have a suitable backup, people might just care then.\n\n/2 cents"}, {"author_name": "Divv", "gravatar_hash": "377fada9f9e7902379b54b43b93a31e8", "author_url": null, "date": "Dec 13, 2009", "message": "There are many more likely scenarios than meteors that may befall your data centre.\n\nYou're trusting someone else with your livelyhood. How do you know they won't have a critical security breach (physical or electronic), or some horrible horrible power failure/surge that fusticates everything.\n\nWhat if some nearby building has a chemical leak, or a terrorist attack or something other crazy situation?\n\nThese kinds of failures are rare, and if you can afford to lose those weeks of data - no sweat, but if you can't, get your offsite backups further away.\n\n\nWhat if Geoff dies/gets sick/quits/becomes malicious etc...? (No offence Geoff)\n\nWhat if the USB harddisk develops the \"Click of death\" and Geoff doesn't notice in the noisy server environment?\n\nDo you have a mirror of Geoff?"}, {"author_name": "Phil", "gravatar_hash": "c7646c4a2cde648b13a3666f818dba4f", "author_url": null, "date": "Dec 13, 2009", "message": "Where I work, offsite (and by offsite we mean &gt;100 miles away, preferably more) is mandatory for each server, in addition to a copy on a separate server in the same data centre and also a copy on S3 - it's a fair amount of BW (10-20GB per server per day) but these days bandwidth is cheap and our servers are on fat pipes with plenty of spare capacity. The S3 backup is the best though as we use rsync to only copy new or updated files and the DB dumps that go up are incremental so you're actually transferring a lot less data than with full dumps that we also store.\n\nIn addition we download and test all backups once a month, retaining those \"known good\" dumps for a year just in case."}, {"author_name": "AjaxUpdater", "gravatar_hash": "8d79c8a762420d5fee633b0732bb5977", "author_url": null, "date": "Dec 13, 2009", "message": "\\Sounds like the solution, then, is to give Fog Creek access to FTP in every week and download the database backups every week?\\\n\nWhy not do this nightly and with automation?  We push to Amazon S3 every night.  Cheap, easy, and offsite.\n\nSeriously you're kind of embarrassing yourself when you balk at a nightly offsite strategy.  It's standard operating procedure everywhere I've ever worked."}, {"author_name": "Joseph", "gravatar_hash": "8a71d77a2ff75e4c5449c21e29c7752f", "author_url": null, "date": "Dec 13, 2009", "message": "Sounds like someone needs to do an FMEA.\n\nhttp://en.wikipedia.org/wiki/Failure_mode_and_effects_analysis"}, {"author_name": "El", "gravatar_hash": "770fa7438a97d6c9f9cf3a79351fcbe7", "author_url": null, "date": "Dec 13, 2009", "message": "Yes, do the backing up. You may be comfortable with the current level of backup, but clearly many others are not; even if the only effect is to increase the comfort level of Joel and trilogy users and Geoff who wouldn't have *quite* as much riding on his ability to get there (what if he's on vacation in France?), given that the cost is low and the human effort is largely one time, why not? \n\nSeriously. Making people feel more comfortable about the continued existence of something they care about is a GOOD thing."}, {"author_name": "Dan", "gravatar_hash": "6c365385b6d0fd8b3489349038c9746c", "author_url": "http://blog.uncommons.org", "date": "Dec 13, 2009", "message": "If we assume that your off-site back-ups are sufficient to recover from any realistic occurrence (fire, flood, etc.), then the only question is how much data are you prepared to lose?\n\nA bi-weekly off-site dump is only acceptable if you can stomach losing two weeks of data.  If losing more than a day's data would hurt too much, then you need to do whatever is necessary to get that data off-site every day."}, {"author_name": "Jesus DeLaTorre", "gravatar_hash": "2864c33de195efc0e94925fa39b26128", "author_url": "http://topreplays.com", "date": "Dec 13, 2009", "message": "Am I the only one thats interested as to how the blogs were recovered? I am *really* interested in looking at these scripts that recovered the blog from google's cache."}, {"author_name": "voyager", "gravatar_hash": "c658f5d2eae05b8e7cab26a40106cd7c", "author_url": null, "date": "Dec 13, 2009", "message": "@Jeff: Daily incremental backups to Fog Creek shouldn't be so heavy... Also, talk to Roland Emmerich. He knows some chinese capable of making secure backups in case of catastrophic Earth events.\n\n@Jesse Hartwick: If the US is destroyed, there is still a whole world out there that would care. Even if most of the site users are USAians.\n\n@Jesus DeLaTorre: If I'm not mistaken, Jeff's scripts got him banned from Google's Cache for a short wile. I don't know if he finally did it by hand. Just try not to ever have to retort to doing that."}, {"author_name": "Jim", "gravatar_hash": "e38a718d0c9dd5a21ac20d5e6416d98a", "author_url": null, "date": "Dec 13, 2009", "message": "Jeff why is your response to blame the hoster rather than take responsibility?  Oh course the hosting provider should have done this and that, but so should YOU.  You think that in business if I told my boss, \"It sucks we lost all our data due to an outage at our hoster.  We pay them to host our stuff, so they should have done backups.  They're terrible!\" that it would fly?\n\nYou additionally rationalize in this post that backup wasn't a priority in this case so you didn't do it. That's absurd.  This failure obviously represented a large pain in the butt for you so I'm guessing you wish it had been a priority.  Why rationalize/justify anything to this extent at all, rather than simply saying, \"Whoops, we didn't make a backup.\""}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Dec 13, 2009", "message": "> Oh course the hosting provider should have done this and that, but so should YOU\n> rather than simply saying, \u201cWhoops, we didn\u2019t make a backup.\u201d\n\nThat is in fact the entire point of this post; that's why we moved the blog VM image to a SO rack server, so it will be subject to our normal backup procedures."}, {"author_name": "Mark", "gravatar_hash": "ea5036994d8e3d00fe4c9ede36c2d05a", "author_url": null, "date": "Dec 13, 2009", "message": "Wow, it takes precisely 5 minutes to set up s3sync and a db dump. Why this isn't being sent to Amazon or Rackspace or even a provisioned server at another hosting provider is absolutely mind-boggling.\n\nOr hell, just buy service from Iron Mountain and forget about it."}, {"author_name": "Jesse Hartwick", "gravatar_hash": "cfd1684ea3df74d506f58377b6da5a72", "author_url": "http://www,appscanadian.ca", "date": "Dec 13, 2009", "message": "@voyager:  \nI was just trying to put Jeffs \"what if Earth blew up / we need lunar backups\" nonsense into proper perspective. \n\nIf the USA is nuked to hell / Earth is destroyed, Jeff can't be blamed for not seeing that coming and for not having made lunar backups. If his server blows up, or his data center burns down and the best he's got is a 3 week old backup on a USB drive, that data loss is on him.\n\nI wasn't trying to portray a US-centric view on this, especially considering I'm Canadian, but rather that Jeff was producing a straw man argument as his defense.\n\n[sarcasm]Also, while international programmers might hate that SO is down if the US is nuked to hell, I think they'd be slightly preoccupied with the World War to care. [/sarcasm]"}, {"author_name": "Jeff Atwood", "gravatar_hash": "51d623f33f8b83095db84ff35e15dbe8", "author_url": "http://www.codinghorror.com/blog/", "date": "Dec 13, 2009", "message": "> Jeff was producing a straw man argument as his defense.\n\nWell, I think the odds of the datacenter burning down to the ground are not particularly far off from global thermonuclear war. They're both highly unlikely to the point of absurdity. Just as you can point to one-in-a-million events of datacenters burning down, I can point to two world wars we've already had. Silly in both cases.\n\nAnd even if we do draw the one short straw in the million, we're <i>still</i> covered. Losing a week or two worth of data for such a rare event is a perfectly fine tradeoff IMO.\n\n(Note that we'd be able to reconstruct this 2-3 week period of data from internet caches if we *really* needed to, as all our pages are 100% public and spidered extremely aggressively.)\n\nAnyway, in the way more likely and realistic failure cases, we're totally covered with the 4am/4pm/12am NAS and daily USB hard drive backups, and the Legal Torrents monthly dumps. And Joel now wants to add weekly backups at Fog Creek, which is no problem."}, {"author_name": "Joel Spolsky", "gravatar_hash": "d77663c217e9d27b725338c06674c94b", "author_url": "http://www.joelonsoftware.com", "date": "Dec 13, 2009", "message": "daily, but yeah."}, {"author_name": "Ryan", "gravatar_hash": "b9745ccf3348a65f990965496ec67317", "author_url": null, "date": "Dec 13, 2009", "message": "Stop being so dramatic. It's not a question of \"who will backup the backups?\". The point is that your \"offsite\" backup strategy relies on a person who _may_ not be available to physically restore or swap out (potentially flaky) external drives.\n\nSo the suggestion was to just backup to another datacenter (S3, Fogcreek, whatever).  It's been established that it's fairly trival to set these sort of things up, and it is indeed better: it's accessible to all members of the team; it's another datacenter; and it doesn't involve some esoteric solution of USB drives and a person living close."}, {"author_name": "Joseph", "gravatar_hash": "8a71d77a2ff75e4c5449c21e29c7752f", "author_url": null, "date": "Dec 13, 2009", "message": "If you are satisfied with the current setup, I say keep it the way it is.  If you ever have a catastrophic failure down the road, then you can reevaluate.\n\nI think the next one is scheduled for 2012."}, {"author_name": "John Topley", "gravatar_hash": "3b687a35025c05dbc7627af428dff0da", "author_url": "http://johntopley.com/", "date": "Dec 13, 2009", "message": "Boy, am I looking forward to this one playing out on the podcast!"}, {"author_name": "Simon Johnson", "gravatar_hash": "dd3f0d1dfd3f41118cd332dbdfa93314", "author_url": "http://www.ckwop.me.uk", "date": "Dec 13, 2009", "message": "Jeff, \n\nIn the UK, one data center got flooded, which nuked the bottom floor of the data centre. Another set on fire wiping out a bunch of racks, and there were incidents of servers being stolen.\n\nYou don't want a daily off-site backup for the eventualities you can invision but more for the ones you can't.\n\nThe simple message from your outage is: \"Don't trust the upstream provider.\"\n\nNow you have to do that for every eventually. You can't trust them to be secure, you can't trust them to be fire resistant, you can't trust them to be flood resilient.\n\nLike any security problem, you have to think as if the datacenter is actively against you and wants to destroy as much of your data as possible. That will put you in the correct mindset to deal with these backup issues."}, {"author_name": "Josh", "gravatar_hash": "bfbc652e45e2041dafac8838c9c50031", "author_url": null, "date": "Dec 14, 2009", "message": "So your data is only worth two cheap USB disks, stored in the same rack as your DB server?"}, {"author_name": "Nick Johnson", "gravatar_hash": "dc69af7dd7151a00843602888638fabb", "author_url": "http://blog.notdot.net/", "date": "Dec 14, 2009", "message": "\"Well, I think the odds of the datacenter burning down to the ground are not particularly far off from global thermonuclear war. They\u2019re both highly unlikely to the point of absurdity. \"\n\nConsidering there's been many cases of datacenter disasters, and exactly zero global thermonuclear wars, this is completely inaccurate.\n\nMore to the point, it doesn't take much imagination to conceive of a disaster that covers at least a one mile radius, encompassing both your primary DC and your only 'offsite' backup. Flooding, earthquakes, and large storms all come to mind. Any sort of localized natural disaster would be enough to eliminate all your backups.\n\nAlso, you're balking at backing up 10G a day? Did I just time warp back to the '90s? If that's a problem, too, you can do incremental backups."}, {"author_name": "Alex", "gravatar_hash": "52085aed08baca3cef862d69b897a98b", "author_url": null, "date": "Dec 14, 2009", "message": "@Josh, Well, he did say the disks are swapped every 2 weeks. They're not just sitting there for fun, they're doing automated backups.\n\nOf course, Nick makes a good point about a natural disaster potentially hitting Geoff's residence, but that's why they're also planning on backing up weekly to FogCreek's data center.\n\nSeems like a fairly solid procedure taking into account likelihoods of disaster and induced data loss."}, {"author_name": "John Topley", "gravatar_hash": "3b687a35025c05dbc7627af428dff0da", "author_url": "http://johntopley.com/", "date": "Dec 14, 2009", "message": "@Alex, Jeff wasn't planning on backing up to Fog Creek's data centre until this comment thread. Jeff was sat with his fingers plugging his ears saying \"Nah nah, I can't hear you\"."}, {"author_name": "Chris Barry", "gravatar_hash": "475ad7c536c1b9d3cd48825ec1b2cd42", "author_url": "http://barryblog.co.uk", "date": "Dec 14, 2009", "message": "@Jeff, I highly recommend the s3 backup solution. I have used Jungledisk to make this process incredibly easy, and currently we are hosting about 15GB of data for $4 a month. \n\nGranted that is without much transfer, but if you were able to backup an uncompressed backup, then jungle disk knows only to transfer the part of the file that changed, so it becomes a diff backup. If you compress your files (as I did at first) it can't move only what has changed, and therefore costs you more. \n\nBut I'm not sure if your dumps are the compressed size or not, so maybe this isn't feasible."}, {"author_name": "Pierre Lebeaupin", "gravatar_hash": "68795690302f37a7119d8e340c73e4c9", "author_url": null, "date": "Dec 14, 2009", "message": "It doesn't seem all that bad, honestly. I think Jeff was more trying to do a satire with his \\how about Earth is destroyed\\ quip. It's not unreasonable to lose a week of data in case of a catastrophic host/colocation failure, especially since it's mostly light user-generated content (as opposed to, say, sales figures, uploaded videos, or MMOG game state), so people can just start over their conversations from where they were one week earlier, it's not like it'll happen often enough to be a problem.\n\nHowever, one potential fault (even with your \\new\\ updates) is that I think all backups should be tested (which means: attempt to restore a working website from it) end to end. It's not enough to do so with the database backups: imagine some issue gets introduced between the database backups and the backups further down the line, what happens if you need to restore from one of these downstream backups? Note, however, that it is still useful to do checks at some intermediate stages, as then you notice earlier if some problem occurs since the downstream stages occur less often.\n\nTo keep talking seriously about a ludicrous (in the context of a web site backup) scenario, I think the Moon isn't really safe either. If a catastrophic event \\just\\ wipes the surface of the planet, and all life as we know it along with it, it's going to be fine; but if Earth itself is obliterated in multiple pieces, at the very least the orbit of the Moon is going to become unpredictable, and it has a non-zero chance of either colliding with the debris or ending in an unstable orbit which will eventually send it into the Sun. So I think Mars is a safer place; the only drawbacks are:\n- very limited bandwidth (I think it's measured in bits of kbit per second)\n- few places for storage (2.5 - the Phoenix lander isn't currently in contact https://twitter.com/MarsPhoenix )\n- hard to get technicians on site - though you never know what could be done if Earth gets obliterated.\nI'm sure there are people in NASA and ESA who think about it totally seriously, though more in the context of a \\memory of mankind\\ type thing. Heck, perhaps you could talk the NASA into attaching a backup drive of SO to their next solar system exploration mission.\n\nOne last thing: we in Europe (Asia/Oceania/your continent here) do mind if the USA get obliterated and we lose every backup of SO as a result, because, sure, in the short term we'll have to worry about the huge repercussions, but in the end we will have to rebuild a whole technology infrastructure, not totally from scratch, but with the need to recover much of the stuff and talent and know-how from the US, and we'll take whatever we can get, including SO.\n\nOh, and make sure no one \\surreptitiously snapp[s] the write-protect tab off of [your] backup tapes\\.... ;) http://archive.gamespy.com/DailyVictim/index.asp?id=692"}, {"author_name": "Kev", "gravatar_hash": "a9840656ceee0faa528806c4c04a9399", "author_url": "http://blog.zygonia.net", "date": "Dec 14, 2009", "message": "\"In the more typical case, where the datacenter became unavailable over the internet, Geoff would drive 1 mile and pull the files off the server or the USB drive. Total elapsed time: a few hours.\"\n\n\"Well, I think the odds of the datacenter burning down to the ground are not particularly far off from global thermonuclear war. They\u2019re both highly unlikely to the point of absurdity.\"\n\nPerhaps, but extremely naive and glib considering the other types of 'unexpected' events that can happen ranging from terrorist activity, chemical spill, fire to severe flooding.\n\nIn the UK severe flooding has been an issue that no data centre company would have considered a realistic service killer. It's now being taken seriously. Even if Jeff does live only 1 mile from the DC, in a severe flood such as we've had in the UK the chances of him and the DC being affected are high and his access to the DC probably restricted.\n\nSee the example cited by another user regarding the cordon put in place after the IRA bombing of the docklands (and please don't suggest a similar event couldn't happen again in the US) and the lack of access even to their off-site backups.\n\nYou should be pushing your crown jewels data off-site more frequently during the day. Trust me, plan properly. Cheap USB disks and hours old stale copies of data is not business continuity planning. Not only that, your operations have that smell of those run and designed by developers, never a good sign. Either take Joel's advice seriously or pay someone to organise this properly for you. Otherwise it makes SO look like it's run by Laurel and Hardy.\n\nIn my company you'd be out of the door faster than you can say your own name with what you've been describing as \"backup\" plans on these blogs over the past months."}, {"author_name": "Scott Koon", "gravatar_hash": "aa1e8277c672507d6a38fa6e60e6ec37", "author_url": "http://lazycoder.com", "date": "Dec 14, 2009", "message": "How often do you back up your SQL Server transaction logs? or are you OK with a possible 12 hour data loss?\n\nI usually recommend 10 minuteT-log backups to a separate spindle, at least, if not a network share on high-transaction systems. Since you've said you have far more reads than writes, you might be able to get away with hourly t-log backups."}, {"author_name": "Scott Koon", "gravatar_hash": "aa1e8277c672507d6a38fa6e60e6ec37", "author_url": "http://lazycoder.com", "date": "Dec 14, 2009", "message": "Err, 4 hour data loss. Not a 12 hour data loss. Math is hard!"}, {"author_name": "Jon B", "gravatar_hash": "c60d994d29c63ea5d83702924b935461", "author_url": null, "date": "Dec 14, 2009", "message": "-Fire\n-Flood\n-Catastrophic equipment failure\n-Human error\n-Sabotage\n\nThese are real disasters that happen to real companies. They are not absurdly unlikely."}, {"author_name": "serbaut", "gravatar_hash": "b4c7a3c412b71d9c7fb01b05aa11f5dd", "author_url": null, "date": "Dec 15, 2009", "message": "It doesn't take that much to loose your data center. A flood will do it: http://www.youtube.com/watch?v=ttcQy3bCiiU"}, {"author_name": "Robin M", "gravatar_hash": "7639ead080da88a1c4b1da9062c4291f", "author_url": null, "date": "Dec 15, 2009", "message": "Just to add to the list of possible datacenter failures that might seem impossible but aren't, how about your uninterruptible power supplies get interupted because they get too hot in the summer\nhttp://www.theregister.co.uk/2006/07/25/level3_sunday_failure/\nor someone sets off the sprinkler system\nhttp://www.theregister.co.uk/2006/03/13/leak_sprinklers/\netc etc\nhttp://www.datacenterknowledge.com/archives/2009/07/06/the-day-after-a-brutal-week-for-uptime/"}, {"author_name": "bofe", "gravatar_hash": "0c51930c02c41fbf1070ee7d902f3476", "author_url": "http://www.bofe.org", "date": "Dec 15, 2009", "message": "Why not start with basics of disaster recovery like Recovery Point Objective and Recovery Time Objective? \n\nLet your requirements drive your choices."}, {"author_name": "Sandeep", "gravatar_hash": "7a42c2b75a02a62f93dafacf0cb4268a", "author_url": "http://sandeep.wordpress.com", "date": "Dec 15, 2009", "message": "Also take a look at the ZFS-Fuse project - it is quite stable right now. I am personally running a few hundred GB of data, off a Samba serverver on a ZFS-Fuse system.\n\nIt makes a great deal of sense to create a mirrored RAIDZ setup (4 drives) and doing incremental backup (using 'zfs send -i' which takes place at the filesystem level).\nOf course you can snapshot very cheaply to guard against software failure.\n\nThe incremental backups guard against hardware failure.\n\nzfs-fuse.net/"}, {"author_name": "jwanagel", "gravatar_hash": "6f5688b4342dc77be8720334dab0fb3a", "author_url": null, "date": "Dec 15, 2009", "message": "The most common scenario for losing all the servers in a rack are human error (versus things like flood or fire).  Specifically, someone mixing up rack numbers and removing then wiping all servers in your rack."}, {"author_name": "K. Brian Kelley", "gravatar_hash": "70351e44f031f245f8e822897ea2fadd", "author_url": "http://www.truthsolutions.com/", "date": "Dec 16, 2009", "message": "All this talk about how Stack Overflow should be handling recovery is all well and good. But for those complaining, there should be something we keep in mind: we don't pay a single cent to use SO's service. So we really have no ownership over the site or its resources. Ultimately, while we can offer suggestions about how we'd do it, it doesn't really matter. As Brent Ozar pointed out in his blog post (at brentozar.com):\n\nhttp://www.brentozar.com/archive/2009/12/designing-a-recovery-strategy-for-stackoverflow/\n\n\"With these answers in mind, StackOverflow\u2019s decisions not to do transaction log backups, offsite log shipping, database mirroring, and so on make good business sense. Us geeks in the crowd may not like it, and we might demand the latest and greatest in backup &amp; recovery technology, but at the same time we want StackOverflow to remain free.\"\n\nThere's something to be said for that."}, {"author_name": "Greg Bray", "gravatar_hash": "da1209aa320f9bbebbf128ac6a0751d3", "author_url": "http://blog.theg2.net/", "date": "Mar 29, 2010", "message": "For reasonably sized backups Amazon S3 works very well. We had to setup off-site backups for about 100 small databases and decided to use a simple Powershell script to zip and encrypt the folder and send it to S3 for off site storage.\n\nhttp://codeblog.theg2.net/2010/02/powershell-7-zip-amazon-s3-upload.html\n\nAnother script takes care of creating the backup folder and clearing out old files, so we only spend about $10-$20 a month for hour-old differential backups with 1 week retention."}]}